[{"path":"index.html","id":"about","chapter":"1 About","heading":"1 About","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports; example, math equation \\(^2 + b^2 = c^2\\).","code":""},{"path":"index.html","id":"usage","chapter":"1 About","heading":"1.1 Usage","text":"chapter contains questions asked relating week course. additional chapter provided miscellaneous question topics. second level heading corresponds one student question group anouncement within week. Third level headings separate question/ anouncement response.","code":""},{"path":"index.html","id":"render-book","chapter":"1 About","heading":"1.2 Render book","text":"can render HTML version example book without changing anything:Find Build pane RStudio IDE, andFind Build pane RStudio IDE, andClick Build Book, select output format, select “formats” ’d like use multiple formats book source files.Click Build Book, select output format, select “formats” ’d like use multiple formats book source files.build book R console:render example PDF bookdown::pdf_book, ’ll need install XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.org/tinytex/.","code":"\nbookdown::render_book()"},{"path":"index.html","id":"preview-book","chapter":"1 About","heading":"1.3 Preview book","text":"work, may start local server live preview HTML book. preview update edit book save individual .Rmd files. can start server work session using RStudio add-“Preview book”, R console:","code":"\nbookdown::serve_book()"},{"path":"supervised-learning-fundamentals.html","id":"supervised-learning-fundamentals","chapter":"2 Supervised Learning Fundamentals","heading":"2 Supervised Learning Fundamentals","text":"Questions relating Week 1: Supervised Learning Fundamentals.","code":""},{"path":"supervised-learning-fundamentals.html","id":"a-question-title","chapter":"2 Supervised Learning Fundamentals","heading":"2.1 A question title","text":"","code":""},{"path":"supervised-learning-fundamentals.html","id":"question","chapter":"2 Supervised Learning Fundamentals","heading":"2.1.1 Question","text":"","code":""},{"path":"supervised-learning-fundamentals.html","id":"response","chapter":"2 Supervised Learning Fundamentals","heading":"2.1.2 Response","text":"","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"3 Linear Regression","heading":"3 Linear Regression","text":"Questions relating Week 2: Linear Regression.","code":""},{"path":"linear-regression.html","id":"residual-variance-denominator","chapter":"3 Linear Regression","heading":"3.1 Residual Variance Denominator","text":"","code":""},{"path":"linear-regression.html","id":"question-1","chapter":"3 Linear Regression","heading":"3.1.1 Question","text":"found literature residual standard error takes form either:\\[ \\hat \\sigma^2 = \\sqrt{\\frac{\\text{RSS}}{n - p}}\\]\\[ \\hat \\sigma = \\sqrt{\\frac{\\text{RSS}}{n - p - 1}}\\]particular, first one introduced Chris’ video whereas second Eq. 3.25 ISLR.Now, noticed \\(F\\)-statistic also uses \\((n-p-1)\\) used latter lab, wondering whether somebody explain difference whether relevant even \\(n>>30\\).","code":""},{"path":"linear-regression.html","id":"response-1","chapter":"3 Linear Regression","heading":"3.1.2 Response","text":"reason difference ISLR just cosmetic: include intercept separate parameter:\\((\\beta_0, \\beta_1, \\ldots, \\beta_p)\\) gives \\(p+1\\) parameters. notes, treat intercept just like parameter, corresponding first column design matrix. means \\(p\\) parameters.different point, going , fact often maximum likelihood estimator variance parameter biased, applications unbiased estimator preferred. e.g. \\[ X_1,\\ldots, X_n \\sim N(\\mu, \\sigma^2)\\]maximum likelihood estimator variance \\[ \\frac{1}{n} \\sum_{= 1}^{n} (X_i - \\bar X)^2,\\]unbiased estimator - one right expectation - need\\[ \\frac{1}{n-1} \\sum_{= 1}^{n} (X_i - \\bar X)^2,\\]Generally sample small enough materially different, probably bigger things worry . ultimately related bias-variance trade-discussed.","code":""},{"path":"linear-regression.html","id":"lab-2-differentiation","chapter":"3 Linear Regression","heading":"3.2 Lab 2 Differentiation","text":"","code":""},{"path":"linear-regression.html","id":"question-2","chapter":"3 Linear Regression","heading":"3.2.1 Question","text":"Lab 2, Section 1.2, understand purpose finding stationary points gradient sign relevant, shouldn’t - 2 equation ?\\[ \\nabla S(\\beta) = 2 X^T(y - X\\beta) = 0.\\]","code":""},{"path":"linear-regression.html","id":"response-2","chapter":"3 Linear Regression","heading":"3.2.2 Response","text":"Thanks , yes right!","code":""},{"path":"linear-regression.html","id":"unbiasedness-of-least-squares-regression","chapter":"3 Linear Regression","heading":"3.3 Unbiasedness of Least Squares Regression","text":"","code":""},{"path":"linear-regression.html","id":"question-3","chapter":"3 Linear Regression","heading":"3.3.1 Question","text":"reading textbook (ESL page 52) linear regression encountered interesting point: Gauss-Markov brings good property BLUE Least Squares estimate, yet seems best linear model choice practical prediction task due bias variance tradeoff. (Figure 3.1)\nFigure 3.1: Exerpt Elements Satistical Learning\nunderstand data noisy (.e., break Gauss-Markov assumptions), Least Squares may overfit perform well high variance.However, question argument imply data good quality (e.g. comply required assumptions), Least Squares estimate may still perform well --sample due high variance?Overall, seems supervised leaning shrugs unbiasedness property. practical value unbiased estimator? inference purpose?","code":""},{"path":"linear-regression.html","id":"response-3","chapter":"3 Linear Regression","heading":"3.3.2 Response","text":"try address questions reverse order.Using unbiased estimator often good thing gives us assurance collect larger larger data sets (assuming true data generating process can represented chosen class models), parameter estimates converge true values. clearly good property estimator .However, inisit estimators must unbiased restricting way. allowing small amount bias may able make better predictions comes cost knowing collect data uncover true data generating process model (even can represented model class).","code":""},{"path":"linear-regression.html","id":"linear-regression-assumptions","chapter":"3 Linear Regression","heading":"3.4 Linear Regression Assumptions","text":"","code":""},{"path":"linear-regression.html","id":"question-4","chapter":"3 Linear Regression","heading":"3.4.1 Question","text":"Hi, better understand linear regression residual diagnosis, study textbook lab materials collect mentioned assumptions get follow-questions. Look forward answers, thanks lot!listed assumptions order importance:Linear additive true mode exists \\(Y\\) \\(X\\), \\(X\\) non-random identifiable (may trivial, formulation); 1. Zero expected mean error terms;Constant variance errors;Errors uncorrelated ;Errors ..d.;Errors normally distributed;Features highly multi-collinear (mandatory assumption).First , make least squares unbiased estimator (based derivation Lab 2), seems need assumption 1 2?Next, assumptions 1+2+3+4 good enough make least squares BLUE. value assumptions 5 6? ISLR textbook seems allude statistical inference purpose, however, based Central Limit Theorem, won’t Normality assumption 6 look unnecessary apart sample efficiency? looks like property least squares MLE introduced Lab2 requires assumptions 1 6.Lastly, multi-collinearity issue increases standard error coefficient correlated features, thus harming statistical inference (according ISLR), yet model still unbiased BLUE/MLE long assumptions hold. multi-collinearity impact prediction ? potentially increase prediction variance can harm overall prediction ability (.e. MSE) well?","code":""},{"path":"linear-regression.html","id":"response-4","chapter":"3 Linear Regression","heading":"3.4.2 Response","text":"first question. Yes, 1 2 required: see can show \\(\\mathbb{E}[\\hat \\beta_i] = \\beta_i\\) coefficients.second question, 2-4 put assumptions first two moments errors may different distributions achieve equal moments. 5 6 additionally assuming entire distribution errors equal distribution Gaussian distribution. Without assumptions know parameter estimates (therefore predictions) sampling distribution also Gaussian limit n becomes large, finite sample sizes don’t know distribution. use CLT motivate Gaussian approximation unknown distribution. making additional assumptions, don’t make approximation: sampling distribution exactly Gaussian finite sample sizes well true limit.final question, gone way answer . including colinear predictors model increases standard errors parameter estimates also increase standard errors predictions, since simply linear combination estimated parameters.","code":""},{"path":"classification-i.html","id":"classification-i","chapter":"4 Classification I","heading":"4 Classification I","text":"Questions relating Week 2: Linear Regression.","code":""},{"path":"classification-i.html","id":"slides-3.3","chapter":"4 Classification I","heading":"4.1 Slides 3.3","text":"","code":""},{"path":"classification-i.html","id":"announcement","chapter":"4 Classification I","heading":"4.1.1 Announcement","text":"Please note slight inconsistency : typed slides say consider 1 covariate intercept speaking hand-written maths consider case two covariates intercept.","code":""},{"path":"classification-i.html","id":"definition-of-distributions-in-labs","chapter":"4 Classification I","heading":"4.2 Definition of distributions in Labs","text":"","code":""},{"path":"classification-i.html","id":"question-5","chapter":"4 Classification I","heading":"4.2.1 Question","text":"trying figure go distributions class definitions . seems like diagonal terms covariance matrix different. particular, \\(Y=2\\) \\(Y=3\\) stretch among \\(x_1\\) \\(x_2\\) axis, second distribution code simply looks like translated normal. provide explanation?`` class 1, ’ll let \\(X\\) standard normal distribution … ’’","code":""},{"path":"classification-i.html","id":"response-5","chapter":"4 Classification I","heading":"4.2.2 Response","text":"Hi, seems like typo snuck lab development. Thanks pointing !Code match stated distributions:Distributions match given code:\\[ X\\ |\\ Y_{ }=2\\ \\sim N\\left(\\left(\\begin{matrix}10\\\\10\\end{matrix}\\right),\\left(\\begin{matrix}1&0\\\\0&1\\end{matrix}\\right)\\right), \\]\\[ X\\ |\\ Y_{ }=3\\ \\sim N\\left(\\left(\\begin{matrix}10\\\\10\\end{matrix}\\right),\\left(\\begin{matrix}1&0\\\\0&16\\end{matrix}\\right)\\right).\\]","code":"\n# make standard normal\nx<-matrix(rnorm(2*n), ncol = 2)\n\n# make class 2\nx[y == 2, ] <- 10 + x[y == 2, ] %*% diag(c(sqrt(10), 1))\n\n# make class 3\nx[y == 3, ] <- -10 + x[y == 3, ] %*% diag(c(1, sqrt(10)))\n\nplot(x[,1],x[,2],col=c(\"orange\",\"blue\",\"green\")[y], asp = 1)"},{"path":"classification-i.html","id":"there-is-also-a-missing-sign-after-pry-1-x-x-approx-hat-px-...","chapter":"4 Classification I","heading":"4.3 There is also a missing \\(=\\) sign after \\(\\Pr(Y = 1 | X = x) \\approx \\hat p(x) ...\\)","text":"","code":""},{"path":"classification-i.html","id":"linear-regression-vs.-lda","chapter":"4 Classification I","heading":"4.4 Linear Regression vs. LDA","text":"","code":""},{"path":"classification-i.html","id":"queston","chapter":"4 Classification I","heading":"4.4.1 Queston","text":"prelude logistic regression section (3.1) week 3 lab, suggests normally distributed covariates equal variances class form basis logistic regression classifier decision boundary straight line.However, linear discriminant analysis also assumes covariates normal shared variance term across classes, decision boundary also straight line. Furthermore, derivations two classifiers build upon Bayes’ theorem log-likelihood., wonder difference two classifiers? /perform differently dataset?","code":""},{"path":"classification-i.html","id":"response-6","chapter":"4 Classification I","heading":"4.4.2 Response","text":"Good question, since two categories two methods can appear similar. two methods present two different approaches problem.Logistic regression describes conditional distribution \\(Y|X=x\\) Bernoulli. One motivation using logistic function (e.g. probit) yields nice properties \\(X\\sim \\text{MVN}\\). saw another motivation videos, considered functions map parameters \\((0,1)\\). distributional assumption X core part method logistic regression.Conversely, LDA assumes \\(X|Y=y \\sim \\text{MVN}\\) basis classification method uses empirical distribution \\(\\text{Pr}(Y=y)\\) distribution interest: \\(Y|X=x\\).means core assumptions LDA broken , example, categorical predictor logistic regression . hand, two possible outcomes, idea LDA extends readily (number possible values \\(y\\) increases 3 ) completely breaks Bernoulli assumption logistic regression.hope helps point differences obvious considering binary outcomes real-valued predictors.","code":""},{"path":"classification-i.html","id":"logistic-regression-assumptions","chapter":"4 Classification I","heading":"4.5 Logistic Regression Assumptions","text":"","code":""},{"path":"classification-i.html","id":"question-6","chapter":"4 Classification I","heading":"4.5.1 Question","text":"reading various literature, derivation logistic regression function, assumption normality - true? covariates free kind distribution?","code":""},{"path":"classification-i.html","id":"response-7","chapter":"4 Classification I","heading":"4.5.2 Response","text":"Great discussion going comments . Hopefully comment LR vs LDA question can help clear things later discussions generalised linear modelling.couple points worth repeating :logistic regression make assumptions marginal distribution X, nice properties X ~ MVN.logistic regression models conditional distribution \\(Y_i|X_i=x_i\\) Bernoulli(\\(p_i\\)). special case Binomial(\\(n\\), \\(p_i\\)) distribution n=1.\n-probability “success” observation \\(\\), \\(p_i\\), modelled (logistic) function linear predictor outcome, equivalently log odds linear combination predictors.","code":""},{"path":"classification-ii.html","id":"classification-ii","chapter":"5 Classification II","heading":"5 Classification II","text":"","code":""},{"path":"classification-ii.html","id":"footnotes","chapter":"5 Classification II","heading":"5.1 Footnotes","text":"Footnotes put inside square brackets caret ^[]. Like one 1.","code":""},{"path":"classification-ii.html","id":"citations","chapter":"5 Classification II","heading":"5.2 Citations","text":"Reference items bibliography file(s) using @key.example, using bookdown package2 (check last code chunk index.Rmd see citation key added) sample book, built top R Markdown knitr3 (citation added manually external file book.bib).\nNote .bib files need listed index.Rmd YAML bibliography key.bs4_book theme makes footnotes appear inline click . example book, added csl: chicago-fullnote-bibliography.csl index.Rmd YAML, include .csl file. download new style, recommend: https://www.zotero.org/styles/RStudio Visual Markdown Editor can also make easier insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations","code":""},{"path":"glms.html","id":"glms","chapter":"6 GLMs","heading":"6 GLMs","text":"","code":""},{"path":"glms.html","id":"irwls","chapter":"6 GLMs","heading":"6.1 IRWLS","text":"","code":""},{"path":"glms.html","id":"question-7","chapter":"6 GLMs","heading":"6.1.1 Question","text":"Hi!two questions related IRWLS method (another unrelated question):picture screenshot slides\nFigure 6.1: Exerpt lecture slides\nwritten equation results \\(\\Sigma_{ii}\\) follows formula (reciprocal slides):\\[ \\Sigma_{ii}=\\left(\\frac{\\partial h}{\\partial\\mu_i}\\right)^2\\phi\\nu_i \\]\nrepeated calculation multiple times keep getting derivative. mind checking whether correct?manually calculating Poisson regression requested lab, re-implemented IRWLS method Bernoulli distribution (steps, calculating derivatives iterating find \\(\\beta\\)s). also checked results match given glm R family=binomial(link=\"logit\"). historical reason (reason) glm refers type regression “binomial” instead “bernoulli”?manually calculating Poisson regression requested lab, re-implemented IRWLS method Bernoulli distribution (steps, calculating derivatives iterating find \\(\\beta\\)s). also checked results match given glm R family=binomial(link=\"logit\"). historical reason (reason) glm refers type regression “binomial” instead “bernoulli”?recommendation good reference (even blog post) read something difference confidence prediction intervals?recommendation good reference (even blog post) read something difference confidence prediction intervals?Thank advance.","code":""},{"path":"glms.html","id":"response-8","chapter":"6 GLMs","heading":"6.1.2 Response","text":"absolutely correct - typo slides partial \\(h\\) respect \\(\\mu_i\\). Apologies spotting presenting editing!absolutely correct - typo slides partial \\(h\\) respect \\(\\mu_i\\). Apologies spotting presenting editing!Bernoulli(p) distribution special case Binomial(n, p) distribution \\(n=1\\). R function deals general case Binomial GLM. assuming user knows relationship, programmer need implement, debug maintain whole function just special case Bernoulli regression (using default link logistic regression).Bernoulli(p) distribution special case Binomial(n, p) distribution \\(n=1\\). R function deals general case Binomial GLM. assuming user knows relationship, programmer need implement, debug maintain whole function just special case Bernoulli regression (using default link logistic regression).don’t good source hand look try get back later week.don’t good source hand look try get back later week.general idea confidence interval describes uncertainty model parameters given model. prediction interval, hand, describes uncertainty future outcome.","code":""},{"path":"glms.html","id":"follow-up-question","chapter":"6 GLMs","heading":"6.1.3 Follow-up Question","text":"Thanks much answers! help lot!Regarding point 3, think something like allow calculation prediction intervals (please correct make mistakes).Step 1) Fit model data set -> model_1Step 2) Sample multivariate normal distribution parameters (classic monte carlo CIs)Step 3) Step 2) obtain point estimate \\(\\hat{\\lambda}\\) mean response Poisson distribution. point estimate, sample \\(n\\) times Poisson first moment values Step 2Step 4) take 5% 95% quantiles build upper lower bounds prediction intervals.explained, prediction intervals “larger” confidence intervals latter take account variability responses. getting something like:\nFigure 6.2: Confidence Interval Plot\n95% confidence interval model :\nFigure 6.3: Prediction Interval Plot\nprediction intervals. bit concerned fact many points lie outside prediction interval though. procedure described flawed?","code":""},{"path":"glms.html","id":"follow-up-response","chapter":"6 GLMs","heading":"6.1.4 Follow-Up Response","text":"uncertainty going two sources. Firstly, outcomes inherently stochastic (e.g Poisson distributed) even knew true model parameters variation new value trying predict. Secondly, don’t know parameters sure, estimate . prediction interval account sources uncertainty. often requires use simulation construct monte-carlo prediction intervals.(Note: also uncertainty whether Poisson model assuming correct, let’s worry now.)hope helps - ’m happy go detail live sessions later week.high-level seems correct, just things wanted comment .small, technical point. Although can simulate n values y lambda estimate, actually need take 1. little unintuitive essentially prediction interval interested combination parameter uncertainty sampling variability new observation. therefore makes better use computational budget push samples way repeatedly simulate using estimated lambda values.small, technical point. Although can simulate n values y lambda estimate, actually need take 1. little unintuitive essentially prediction interval interested combination parameter uncertainty sampling variability new observation. therefore makes better use computational budget push samples way repeatedly simulate using estimated lambda values.general make sense compare widths CIs prediction intervals way, since parameters outcomes usually scale (though Poisson ). right idea though, confidence interval expected value new outcome going narrower prediction interval new outcome. (also similar point, unrelated, point make prediction intervals including parameter uncertainty wider assuming point estimate correct.)general make sense compare widths CIs prediction intervals way, since parameters outcomes usually scale (though Poisson ). right idea though, confidence interval expected value new outcome going narrower prediction interval new outcome. (also similar point, unrelated, point make prediction intervals including parameter uncertainty wider assuming point estimate correct.)right concerned many point lie outside prediction interval suggests additional source variability data modelling. (Hopefully enough hint get started right direction!)right concerned many point lie outside prediction interval suggests additional source variability data modelling. (Hopefully enough hint get started right direction!)","code":""},{"path":"glms.html","id":"centering-predictors-in-glms","chapter":"6 GLMs","heading":"6.2 Centering Predictors in GLMs","text":"","code":""},{"path":"glms.html","id":"question-8","chapter":"6 GLMs","heading":"6.2.1 Question","text":"last video week, pre-processing cloth data, center predictor cloth length? purely ease interpretation parameters later something else missed?","code":""},{"path":"glms.html","id":"response-9","chapter":"6 GLMs","heading":"6.2.2 Response","text":"couple reasons (’m sure , let’s stick main two).Firstly, yes ease interpreteation important - center cloth length \\(\\exp(\\beta_0)\\) interpretation expected number defects cloth length 0m. Using centered length gives us expected number defects cloth average length, something actually care . (mention lab solutions)Secondly, makes inference process easier. Centering predictor helps separate “orthogonalise” effects slope intercept parameters, thereby reducing covariance corresponding parameter estimates. reduces much uncertainty one parameter changes based estimated value parameter. parameters re fully orthogonal, uncertainty one effected value .terms variance matrix \\(\\text{Var}(\\mathbf{\\beta})\\), centering covariate reduces size -diagonal terms. means uncertainty slope impact uncertainty intercept quite much, vice-versa. benefits can seen clearly consider variance estimated linear predictor cloth length \\(\\):\\[\\text{Var}(\\hat \\beta_0 + \\hat \\beta_1) = \\text{Var}(\\hat \\beta_0) + ^2 \\text{Var}(\\hat \\beta_2) + 2a \\text{Cov}(\\hat \\beta_0, \\hat \\beta_1)\\]going smaller can use centered covariate less dependence parameter estimates.","code":""},{"path":"glms.html","id":"follow-up-question-1","chapter":"6 GLMs","heading":"6.2.3 Follow-up Question","text":"Thanks. However, ’m sure understand second point making. Can elaborate exactly mean “centering predictor helps separate orthogonalise effects slope intercept parameters”? right say centered predictor, since intercept (exp ) expected value \\(y\\) average \\(x\\), variance smallest, almost independent slope?Moreover, centering required step rather convenient “nice--”, yes?","code":""},{"path":"glms.html","id":"follow-up-response-1","chapter":"6 GLMs","heading":"6.2.4 Follow-up Response","text":"Yes, right say centered predictor makes \\(\\hat \\beta_0\\) \\(\\hat \\beta_1\\) almost independent (least less dependent using raw covariate).can see clearly linear regression diagram . covariate takes large values even small change slope lead large change intercept using raw covariate. using centered version covariate dependence much reduced.\nFigure 6.4: Schematic plot showing effect centering dependence parameter estimates.\n","code":""},{"path":"resampling-and-model-selection.html","id":"resampling-and-model-selection","chapter":"7 Resampling and Model Selection","heading":"7 Resampling and Model Selection","text":"","code":""},{"path":"resampling-and-model-selection.html","id":"equations","chapter":"7 Resampling and Model Selection","heading":"7.1 Equations","text":"equation.\\[\\begin{equation} \n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  \\tag{7.1}\n\\end{equation}\\]may refer using \\@ref(eq:binom), like see Equation (7.1).","code":""},{"path":"resampling-and-model-selection.html","id":"theorems-and-proofs","chapter":"7 Resampling and Model Selection","heading":"7.2 Theorems and proofs","text":"Labeled theorems can referenced text using \\@ref(thm:tri), example, check smart theorem 7.1.Theorem 7.1  right triangle, \\(c\\) denotes length hypotenuse\n\\(\\) \\(b\\) denote lengths two sides, \n\\[^2 + b^2 = c^2\\]Read https://bookdown.org/yihui/bookdown/markdown-extensions--bookdown.html.","code":""},{"path":"resampling-and-model-selection.html","id":"callout-blocks","chapter":"7 Resampling and Model Selection","heading":"7.3 Callout blocks","text":"bs4_book theme also includes special callout blocks, like .rmdnote.can use markdown inside block.user define appearance blocks LaTeX output.may also use: .rmdcaution, .rmdimportant, .rmdtip, .rmdwarning block name.R Markdown Cookbook provides help use custom blocks design callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html","code":"\nhead(beaver1, n = 5)\n#>   day time  temp activ\n#> 1 346  840 36.33     0\n#> 2 346  850 36.34     0\n#> 3 346  900 36.35     0\n#> 4 346  910 36.42     0\n#> 5 346  920 36.55     0"},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"8 Tree-based Methods","heading":"8 Tree-based Methods","text":"","code":""},{"path":"tree-based-methods.html","id":"publishing","chapter":"8 Tree-based Methods","heading":"8.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"tree-based-methods.html","id":"pages","chapter":"8 Tree-based Methods","heading":"8.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"tree-based-methods.html","id":"metadata-for-sharing","chapter":"8 Tree-based Methods","heading":"8.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"},{"path":"regularisation-and-pcr.html","id":"regularisation-and-pcr","chapter":"9 Regularisation and PCR","heading":"9 Regularisation and PCR","text":"","code":""},{"path":"regularisation-and-pcr.html","id":"publishing-1","chapter":"9 Regularisation and PCR","heading":"9.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"regularisation-and-pcr.html","id":"pages-1","chapter":"9 Regularisation and PCR","heading":"9.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"regularisation-and-pcr.html","id":"metadata-for-sharing-1","chapter":"9 Regularisation and PCR","heading":"9.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"},{"path":"non-parametric-regression.html","id":"non-parametric-regression","chapter":"10 Non-parametric Regression","heading":"10 Non-parametric Regression","text":"","code":""},{"path":"non-parametric-regression.html","id":"publishing-2","chapter":"10 Non-parametric Regression","heading":"10.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"non-parametric-regression.html","id":"pages-2","chapter":"10 Non-parametric Regression","heading":"10.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"non-parametric-regression.html","id":"metadata-for-sharing-2","chapter":"10 Non-parametric Regression","heading":"10.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"},{"path":"svm-and-kernel-methods.html","id":"svm-and-kernel-methods","chapter":"11 SVM and Kernel Methods","heading":"11 SVM and Kernel Methods","text":"","code":""},{"path":"svm-and-kernel-methods.html","id":"publishing-3","chapter":"11 SVM and Kernel Methods","heading":"11.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"svm-and-kernel-methods.html","id":"pages-3","chapter":"11 SVM and Kernel Methods","heading":"11.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"svm-and-kernel-methods.html","id":"metadata-for-sharing-3","chapter":"11 SVM and Kernel Methods","heading":"11.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"},{"path":"writing-reports.html","id":"writing-reports","chapter":"12 Writing Reports","heading":"12 Writing Reports","text":"","code":""},{"path":"writing-reports.html","id":"including-code-in-a-latex-document","chapter":"12 Writing Reports","heading":"12.1 Including Code in a LaTeX document","text":"wish include code within assessment appendix report\ncan using either code listing verbatim environment.\nNote standard practice include raw code output within body report.recommend including body description function within relevant question using verbatim code listing environment.Overleaf Docs Code ListingsOverlead Docs Verbatim Environments","code":""},{"path":"writing-reports.html","id":"publishing-4","chapter":"12 Writing Reports","heading":"12.2 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"writing-reports.html","id":"pages-4","chapter":"12 Writing Reports","heading":"12.3 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"writing-reports.html","id":"metadata-for-sharing-4","chapter":"12 Writing Reports","heading":"12.4 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"},{"path":"resources.html","id":"resources","chapter":"13 Resources","heading":"13 Resources","text":"","code":""},{"path":"resources.html","id":"statistics-and-statistical-inference-books","chapter":"13 Resources","heading":"13.1 Statistics and Statistical Inference Books","text":"Several students asked recommendations resources improve background knowledge statistical inference modelling. options may may suitable, depeding current level trying achieve.given brief description level target audience text. However, quick skim section topic already know (linear regression) usually way assess book going good fit useful .know gems included list, feel free mention responses!Mathematical Statistics Data analysis - John . RiceThis text covers basics probability statistics usually contained first couple undergraduate stats courses. Generally first university courses bit dry, building required knowledge interesting things. book slightly better average treatment terms readability fairly comprehensive, making well suited reference text stuff might known now forgotten, never studied .Essential medical statistics - Betty Kirkwood Jonathan SterneThis book focuses advanced topics statistisc, inference, hypothesis testing modelling, applications perspective. applications uses medical statistics, authors give sufficient context need familiar context reading. readable book, moderate amount mathematical detail.Core Statistics - Simon WoodThis book gives introduction core topics statistics aimed new graduate-level students. mathematically dense written apprachable manner (unsurprisingly) covers core ideas statistics. means often good source get overview topic cover key points area quickly. probably wise supplement readings applied see worked examples detailed text hand topics need explaining greater detail.Likelihood - Yudi PawitanThis book focuses entirely likelihood inference covers theory applications great deal detail. highly recommend supplement frequentist topics covered core statistics elements statistical learning. build little assumed knowledge also goes cover advanced topics later chapters.Kendall’s advanced theory statistics.alternative Likelihood, aimed similar audience level. Split several volumes good deep-dive particular topic probably one try read cover cover!Bayesian StatisticsYou don’t need worry now, since considering frequentist approaches inference course. However, remiss include Bayesian texts leave impression classical frequentist approaches statistics option. Many topics cover can considered Bayesian perspective, instead considering model parameters fixed unknown quantities, instead consider random variables use probability distributions describe beliefs values.following books may useful Bayesian inference course. former theoretical latter applied.Kendall’s advanced theory statistics. Vol. 2B, Bayesian inference.Bayesian Data Analysis - Gelman et al.","code":""},{"path":"resources.html","id":"publishing-5","chapter":"13 Resources","heading":"13.2 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"resources.html","id":"pages-5","chapter":"13 Resources","heading":"13.3 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"resources.html","id":"metadata-for-sharing-5","chapter":"13 Resources","heading":"13.4 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
